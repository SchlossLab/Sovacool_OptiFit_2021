""" Benchmarking the OptiFit algorithm using an external reference database """
import itertools
import shutil

wildcard_constraints:
    seed='\d+',
    region='bact_v4'

subworkflow prep_db:
    workdir:
        "../0_prep_db/"
subworkflow prep_samples:
    workdir:
        "../1_prep_samples/"

datasets = ["soil"]#"human", "marine", "mouse", ]
refs = ["silva", "gg", "rdp"]
regions = ['bact_v4']
methods = ["open", "closed"]
printrefs = ['f']
seeds = range(1,2)#11)


rule fit_ref_db_targets:
    input:
        "results/sensspec.tsv",
        "results/benchmarks.tsv",
        'results/fraction_reads_mapped.tsv',
        'results/ref_sizes.tsv'

rule copy_preclust_inputs:  # because filter.seqs doesn't like hyphens in filepaths
    input:
        filter=prep_samples("data/{dataset}/processed/{dataset}.filter"),
        fasta=prep_samples("data/{dataset}/processed/{dataset}.fasta"),
        count=prep_samples("data/{dataset}/processed/{dataset}.count_table"),
        fasta_db=prep_db("data/{ref}/{ref}.{region}.fasta"),
        tax=prep_db("data/{ref}/{ref}.bacteria.tax")
    output:
        filter="data/{dataset}_{ref}_{region}/inputs/{dataset}.filter",
        fasta="data/{dataset}_{ref}_{region}/inputs/{dataset}.fasta",
        count="data/{dataset}_{ref}_{region}/inputs/{dataset}.count_table",
        fasta_db="data/{dataset}_{ref}_{region}/inputs/{ref}.{region}.fasta",
        tax="data/{dataset}_{ref}_{region}/inputs/{ref}.bacteria.tax"
    params:
        outdir="data/{dataset}_{ref}_{region}/inputs/"
    shell:
        """
        cp {input.filter} {output.filter}
        cp {input.fasta} {output.fasta}
        cp {input.count} {output.count}
        cp {input.fasta_db} {output.fasta_db}
        cp {input.tax} {output.tax}
        """

rule preclust_db:
    input:
        fasta_db=rules.copy_preclust_inputs.output.fasta_db,
        fasta_sample=rules.copy_preclust_inputs.output.fasta,
        tax=rules.copy_preclust_inputs.output.tax,
        hard_filter=rules.copy_preclust_inputs.output.filter
    output:
        fasta="data/{dataset}_{ref}_{region}/preclust_db/{ref}.{region}.filter.unique.precluster.fasta",
        count="data/{dataset}_{ref}_{region}/preclust_db/{ref}.{region}.filter.unique.precluster.pick.count_table",
        tax="data/{dataset}_{ref}_{region}/preclust_db/{ref}.{region}.pick.tax",
        dist="data/{dataset}_{ref}_{region}/preclust_db/{ref}.{region}.filter.unique.precluster.dist"
    params:
        tax="data/{dataset}_{ref}_{region}/preclust_db/{ref}.bacteria.pick.tax",
        outdir="data/{dataset}_{ref}_{region}/preclust_db/"
    resources:
        procs=12,
        walltime_hrs=48
    log:
        "log/{dataset}_{ref}_{region}/preclust_db.log"
    benchmark:
        "benchmarks/{dataset}_{ref}_{region}/preclust_db.txt"
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
        set.current(processors={resources.procs});
        
        filter.seqs(fasta={input.fasta_db}, hard={input.hard_filter});
        unique.seqs(fasta=current, format=count);
        pre.cluster(fasta=current, name=current, diffs=2, processors={resources.procs});
        dist.seqs(fasta=current, cutoff=0.03, processors={resources.procs});

        list.seqs(fasta=current);
        get.seqs(accnos=current, taxonomy={input.tax});
        get.seqs(accnos=current, count=current);
        summary.seqs(fasta=current, count=current);
        rename.file(input={params.tax}, new={output.tax}) '
        """

rule cluster_db:
    input:
        dist=rules.preclust_db.output.dist,
        count=rules.preclust_db.output.count
    output:
        list="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/{ref}.{region}.filter.unique.precluster.opti_mcc.list",
        steps="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/{ref}.{region}.filter.unique.precluster.opti_mcc.steps",
        sensspec="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/{ref}.{region}.filter.unique.precluster.opti_mcc.sensspec"
    params:
        outdir="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/"
    log:
        "log/{dataset}_{ref}_{region}/cluster_db.seed_{seed}.log"
    benchmark:
        "benchmarks/{dataset}_{ref}_{region}/cluster_db.seed_{seed}.txt"
    resources:
        procs=8
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
        set.seed(seed={wildcards.seed});
        set.current(processors={resources.procs});
        cluster(column={input.dist}, count={input.count}, cutoff=0.03) '
        """

rule get_otu_reps_db:
    input:
        list=rules.cluster_db.output.list,
        count=rules.preclust_db.output.count,
        tax=rules.preclust_db.output.tax,
        dist=rules.preclust_db.output.dist
    output:
        tax="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/{ref}.{region}.seed_{seed}.opti_mcc.0.03.cons.taxonomy",
        sum="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/{ref}.{region}.seed_{seed}.opti_mcc.0.03.cons.tax.summary",
        count="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/{ref}.{region}.seed_{seed}.opti_mcc.0.03.rep.count_table"
    params:
        outdir="results/{dataset}_{ref}_{region}/cluster_db/seed_{seed}/"
    log:
        "log/get_otu_reps.{dataset}_{ref}_{region}.seed_{seed}.log"
    benchmark:
        "benchmarks/get_otu_reps.{dataset}_{ref}_{region}.seed_{seed}.txt"
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
        classify.otu(list={input.list}, count={input.count}, taxonomy={input.tax}, label=0.03);
        get.oturep(column={input.dist}, list={input.list}, count={input.count}, method=abundance) '
        """

rule combine_seqs:
    input:
        fasta_sample=rules.copy_preclust_inputs.output.fasta,
        fasta_db=rules.preclust_db.output.fasta,
        count_sample=rules.copy_preclust_inputs.output.count,
        count_db=rules.preclust_db.output.count
    output:
        fasta_all="data/{dataset}_{ref}_{region}/combined_seqs/{dataset}_{ref}.{region}.all.fasta",
        count_all="data/{dataset}_{ref}_{region}/combined_seqs/{dataset}_{ref}.{region}.all.count_table",
        accnos_refs="data/{dataset}_{ref}_{region}/combined_seqs/{ref}.{region}.filter.unique.precluster.accnos"
    params:
        outdir="data/{dataset}_{ref}_{region}/combined_seqs/"
    log:
        "log/{dataset}_{ref}_{region}/combine_seqs.log"
    benchmark:
        "benchmarks/{dataset}_{ref}_{region}/combine_seqs.txt"
    shell:
        """
        cat {input.fasta_sample} {input.fasta_db} > {output.fasta_all}
        cat {input.count_sample} {input.count_db} > {output.count_all}
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
            list.seqs(fasta={input.fasta_sample});
            list.seqs(fasta={input.fasta_db})
        '
        """

rule calc_dists_combined:
    input:
        fasta=rules.combine_seqs.output.fasta_all
    output:
        column="data/{dataset}_{ref}_{region}/combined_seqs/{dataset}_{ref}.{region}.all.dist"
    params:
        outdir="data/{dataset}_{ref}_{region}/combined_seqs/"
    log:
        "log/{dataset}_{ref}_{region}/calc_dists_combined.log"
    benchmark:
        "benchmarks/{dataset}_{ref}_{region}/calc_dists_combined.txt"
    resources:
        procs=16
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
            dist.seqs(fasta={input.fasta}, cutoff=0.03, processors={resources.procs}) '
        """

rule cluster_combined:
    input:
        dist=rules.calc_dists_combined.output.column,
        count=rules.combine_seqs.output.count_all
    output:
        list="results/{dataset}_{ref}_{region}/cluster_combined/seed_{seed}/{dataset}_{ref}.{region}.all.opti_mcc.list",
        steps="results/{dataset}_{ref}_{region}/cluster_combined/seed_{seed}/{dataset}_{ref}.{region}.all.opti_mcc.steps",
        sensspec='results/{dataset}_{ref}_{region}/cluster_combined/seed_{seed}/{dataset}_{ref}.{region}.all.opti_mcc.sensspec'
    params:
        outdir="results/{dataset}_{ref}_{region}/cluster_combined/seed_{seed}/"
    log:
        "log/{dataset}_{ref}_{region}/cluster_combined.seed_{seed}.log"
    benchmark:
        "benchmarks/{dataset}_{ref}_{region}/cluster_combined.seed_{seed}.txt"
    resources:
        procs=8
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.outdir});
        set.seed(seed={wildcards.seed});
        set.current(processors={resources.procs});
        cluster(column={input.dist}, count={input.count}, cutoff=0.03) '
        """

rule fit_to_ref_db:
    input:
        fasta=rules.combine_seqs.output.fasta_all,
        count=rules.combine_seqs.output.count_all,
        column=rules.calc_dists_combined.output.column,
        reflist=rules.cluster_db.output.list
    output:
        sensspec='results/{dataset}_{ref}_{region}/optifit/method_{method}/printref_{printref}/seed_{seed}/{dataset}_{ref}.{region}.all.optifit_mcc.sensspec',
        list='results/{dataset}_{ref}_{region}/optifit/method_{method}/printref_{printref}/seed_{seed}/{dataset}_{ref}.{region}.all.optifit_mcc.list',
    params:
        outdir="results/{dataset}_{ref}_{region}/optifit/method_{method}/printref_{printref}/seed_{seed}/"
    benchmark:
        "benchmarks/{dataset}_{ref}_{region}/optifit.method_{method}.printref_{printref}.seed_{seed}.txt"
    log:
        "log/{dataset}_{ref}_{region}/optifit.method_{method}.printref_{printref}.seed_{seed}.log"
    resources:
        procs=8
    shell:
        """
        mothur "#set.logfile(name={log}); set.dir(output={params.outdir});
        set.seed(seed={wildcards.seed});
        set.current(processors={resources.procs});
        cluster.fit(reflist={input.reflist}, fasta={input.fasta}, count={input.count}, column={input.column},  printref={wildcards.printref}, method={wildcards.method}) "
        """

rule reformat_optifit_results:  # for plotting
    input:
        sensspec=rules.fit_to_ref_db.output.sensspec,
        bench=rules.fit_to_ref_db.benchmark,
        code="../../code/R/reformat_optifit_results.R"
    output:
        sensspec='results/{dataset}_{ref}_{region}/optifit/method_{method}/printref_{printref}/seed_{seed}/{dataset}_{ref}.{region}.all.optifit_mcc.mod.sensspec',
        bench='benchmarks/{dataset}_{ref}_{region}/optifit.method_{method}.printref_{printref}.seed_{seed}.mod.txt'
    benchmark:
        "benchmarks/{dataset}_{ref}_{region}/reformat_optifit_results.method_{method}.printref_{printref}.seed_{seed}.txt"
    params:
        dataset="{dataset}",
        ref="{ref}",
        region="{region}",
        seed="{seed}",
        printref='{printref}',
        method='{method}'
    script:
        "../../code/R/reformat_optifit_results.R"

rule fraction_reads_mapped:
    input:
        list=rules.fit_to_ref_db.output.list,
        count=rules.copy_preclust_inputs.output.count
    output:
        txt='results/{dataset}_{ref}_{region}/optifit/method_{method}/printref_{printref}/seed_{seed}/fraction_reads_mapped.txt'
    wildcard_constraints:
        method='closed'
    run:
        with open(input.count, 'r') as count_file: # first column of all lines except first line
            next(count_file)
            all_reads = {line.split('\t')[0] for line in count_file}
        with open(input.list, 'r') as list_file: # third column onward of second line, each seq in each OTU delimited by comma
            next(list_file)
            line = next(list_file)
            mapped_reads = {seq for column in line.split()[2:] for seq in column.split(',')}
        fraction_mapped = round(len(mapped_reads.intersection(all_reads)) / len(all_reads), 3)
        with open(output.txt, 'w') as output_file:
            output_file.write(f"{wildcards.dataset}\t{wildcards.ref}\t{wildcards.region}\t{wildcards.seed}\t{wildcards.method}\t{wildcards.printref}\t{fraction_mapped}\n")

rule merge_mapped_stats:
    input:
        expand('results/{dataset}_{ref}_{region}/optifit/method_closed/printref_{printref}/seed_{seed}/fraction_reads_mapped.txt',
        dataset=datasets, ref=refs, region=regions, printref=printrefs, seed=seeds)
    output:
        txt='results/fraction_reads_mapped.tsv'
    shell:
        """
        echo "dataset\tref\tregion\tseed\tmethod\tprintref\tfraction_mapped\n" > {output.txt}
        cat {input} >> {output.txt}
        """

rule merge_results2:
    input:
        sensspec=expand('results/{dataset}_{ref}_{region}/optifit/method_{method}/printref_{printref}/seed_{seed}/{dataset}_{ref}.{region}.all.optifit_mcc.mod.sensspec',
                dataset=datasets, ref=refs, region=regions, seed=seeds, method=methods, printref=printrefs),
        bench=expand('benchmarks/{dataset}_{ref}_{region}/optifit.method_{method}.printref_{printref}.seed_{seed}.mod.txt',
            dataset=datasets, ref=refs, region=regions, seed=seeds, method=methods, printref=printrefs),
        code='../../code/R/merge_results.R'
    output:
        sensspec="results/sensspec.tsv",
        bench="results/benchmarks.tsv"
    benchmark:
        "benchmarks/merge_results.txt"
    script:
        '../../code/R/merge_results.R'

def count_seqs(filename):
    with open(filename, 'r') as infile:
        num_seqs = sum(1 for line in infile if line.startswith('>'))
    return num_seqs

rule count_ref_sizes:
    input:
        fasta=expand("data/{dataset}_{ref}_{region}/preclust_db/{ref}.{region}.filter.unique.precluster.fasta", dataset=datasets, ref=refs, region=regions)
    output:
        txt='results/ref_sizes.tsv'
    benchmark:
        'benchmarks/count_ref_sizes.txt'
    run:
        with open(output.txt, 'w') as outfile:
            outfile.write('reference\tregion\tnum_seqs\tdataset_filter\n')
            for filename in input.fasta:
                parameters = filename.split('/')[1].split('_')
                num_seqs = count_seqs(filename)
                dataset = parameters[0]
                ref = parameters[1]
                region = parameters[3]
                outfile.write(f"{ref}\t{region}\t{num_seqs}\t{dataset}\n")
