import os

subworkflow prep_db:
    workdir:
        "../0_prep_db/"

wildcards = glob_wildcards("data/{dataset}/SRR_Acc_List.txt")

def get_sra_list(dataset):
    with open(f"data/{dataset}/SRR_Acc_List.txt", 'r') as file:
        sra = [line.strip() for line in file]
    return sra

sra_list = {dataset: get_sra_list(dataset) for dataset in ("human", "marine", "soil")}
seeds = range(10)

for dataset in wildcards.dataset:
    include: f"code/{dataset}.smk"

rule preprocess_targets:
    input:
        expand("results/{dataset}/{ref}/{dataset}.seed_{seed}.opti_mcc.{ext}", dataset=('soil',), ref=("silva",), seed=seeds, ext=('list', 'steps', 'sensspec')),
        [f"data/{dataset}/raw/{sra}_{i}.fastq.gz" for dataset in ("soil",) for sra in sra_list[dataset] for i in (1,2)]

rule download_most:
    input:
        list="data/{dataset}/SRR_Acc_List.txt",
        sh="code/download.sh"
    output:
        fastq=expand("data/{{dataset}}/raw/{{SRA}}_{i}.fastq.gz", i=(1,2))
    benchmark:
        "benchmarks/{dataset}/download_{SRA}.txt"
    params:
        sra="{SRA}",
        outdir="data/{dataset}/raw"
    wildcard_constraints:
        dataset="human|marine|soil"
    shell:
        """
        bash {input.sh} {params.sra} {params.outdir}
        """

rule preprocess:
    input:
        file="data/{dataset}/{dataset}.files",
        align_db=prep_db("data/{ref}/{ref}.bact_v4.fasta"),
        rdp_fasta=prep_db("data/rdp/rdp.fasta"),
        rdp_tax=prep_db("data/rdp/rdp.tax")
    output:
        filter="data/{dataset}/{ref}/processed/{dataset}.filter",
        count="data/{dataset}/{ref}/processed/{dataset}.count_table",
        tax="data/{dataset}/{ref}/processed/{dataset}.tax",
        fasta="data/{dataset}/{ref}/processed/{dataset}.fasta"
    params:
        workdir="data/{dataset}/{ref}/temp/",
        filter="data/{dataset}/{ref}/temp/{dataset}.filter",
        count="data/{dataset}/{ref}/temp/{dataset}.trim.contigs.good.unique.good.filter.unique.precluster.denovo.uchime.pick.pick.count_table",
        tax="data/{dataset}/{ref}/temp/{dataset}.trim.contigs.good.unique.good.filter.unique.precluster.pick.rdp.wang.pick.taxonomy",
        fasta="data/{dataset}/{ref}/temp/{dataset}.trim.contigs.good.unique.good.filter.unique.precluster.pick.pick.fasta",
        trim_contigs='data/{dataset}/{ref}/temp/{dataset}.trim.contigs.fasta',
        contigs_groups='data/{dataset}/{ref}/temp/{dataset}.contigs.groups'
    log:
        "log/{dataset}/{ref}/preprocess.log"
    benchmark:
        "benchmarks/{dataset}/{ref}/preprocess.txt"
    resources:
        procs=12,
        pmem=8
    shell:
        """
        mothur '#set.logfile(name={log});
            set.dir(input=data/{wildcards.dataset}/raw, output={params.workdir});
            make.contigs(inputdir=data/{wildcards.dataset}/raw, file={input.file}, processors={resources.procs});
            set.dir(input=data/{wildcards.dataset}/raw, output={params.workdir});
            screen.seqs(fasta={params.trim_contigs}, group={params.contigs_groups}, maxambig=0, maxlength=275, maxhomop=8);
            unique.seqs();
            count.seqs(name=current, group=current);
            align.seqs(fasta=current, reference={input.align_db}, processors={resources.procs});
            screen.seqs(fasta=current, count=current, start=1968, end=11550);
            filter.seqs(fasta=current, vertical=T, trump=.);
            unique.seqs(fasta=current, count=current);
            pre.cluster(fasta=current, count=current, diffs=2);
            chimera.uchime(fasta=current, count=current, dereplicate=T);
            remove.seqs(fasta=current, accnos=current);
            classify.seqs(fasta=current, count=current, reference={input.rdp_fasta}, taxonomy={input.rdp_tax}, cutoff=80);
            remove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota); '
        cp {params.filter} {output.filter}
        cp {params.count} {output.count}
        cp {params.tax} {output.tax}
        cp {params.fasta} {output.fasta}
        """

rule calc_dists_samples:
    input:
        fasta=rules.preprocess.output.fasta,
    output:
        dist="results/{dataset}/{ref}/{dataset}.dist"
    params:
        workdir='results/{dataset}/{ref}/'
    log:
        "log/{dataset}/{ref}/calc_dists.log"
    benchmark:
        "benchmarks/{dataset}/{ref}/calc_dists.txt"
    resources:
        procs=12,
        walltime_hrs=48
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.workdir});
        dist.seqs(fasta={input.fasta}, cutoff=0.03, processors={resources.procs}) '
        """

rule cluster_samples:
    input:
        dist=rules.calc_dists_samples.output.dist,
        count=rules.preprocess.output.count
    output:
        list="results/{dataset}/{ref}/{dataset}.seed_{seed}.opti_mcc.list",
        steps="results/{dataset}/{ref}/{dataset}.seed_{seed}.opti_mcc.steps",
        sensspec="results/{dataset}/{ref}/{dataset}.seed_{seed}.opti_mcc.sensspec",
        workdir=directory("results/{dataset}/{ref}/tmp/seed_{seed}/")
    params:
        dist="results/{dataset}/{ref}/tmp/seed_{seed}/{dataset}.dist",
        count="results/{dataset}/{ref}/tmp/seed_{seed}/{dataset}.count_table",
        list="results/{dataset}/{ref}/tmp/seed_{seed}/{dataset}.opti_mcc.list",
        steps="results/{dataset}/{ref}/tmp/seed_{seed}/{dataset}.opti_mcc.steps",
        sensspec="results/{dataset}/{ref}/tmp/seed_{seed}/{dataset}.opti_mcc.sensspec"
    log:
        "log/{dataset}/{ref}/cluster.seed_{seed}.log"
    benchmark:
        "benchmarks/{dataset}/{ref}/cluster.seed_{seed}.txt"
    shell:
        """
        cp {input.dist} {params.dist}
        cp {input.count} {params.count}
        mothur '#set.logfile(name={log}); set.dir(input={output.workdir}, output={output.workdir});
        set.seed(seed={wildcards.seed});
        cluster(column={params.dist}, count={params.count}, cutoff=0.03) '
        mv {params.list} {output.list}
        mv {params.steps} {output.steps}
        mv {params.sensspec} {output.sensspec}
        """

rule get_otu_reps_samples:
    input:
        list=rules.cluster_samples.output.list,
        count=rules.preprocess.output.count,
        tax=rules.preprocess.output.tax,
        dist=rules.calc_dists_samples.output.dist
    output:
        tax="results/{dataset}/{ref}/{dataset}.seed_{seed}.opti_mcc.0.03.cons.taxonomy",
        sum="results/{dataset}/{ref}/{dataset}.seed_{seed}.opti_mcc.0.03.cons.tax.summary",
        count="results/{dataset}/{ref}/{dataset}.seed_{seed}.opti_mcc.0.03.rep.count_table"
    params:
        workdir="results/{dataset}/{ref}/"
    log:
        "log/{dataset}/{ref}/get_otu_reps.seed_{seed}.log"
    benchmark:
        "benchmarks/{dataset}/{ref}/get_otu_reps.seed_{seed}.txt"
    shell:
        """
        mothur '#set.logfile(name={log}); set.dir(output={params.workdir});
        classify.otu(list={input.list}, count={input.count}, taxonomy={input.tax}, label=0.03);
        get.oturep(column={input.dist}, list={input.list}, count={input.count}, method=abundance) '
        """
